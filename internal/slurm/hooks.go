package slurm

import (
	"fmt"
	"os"
	"path/filepath"
	"strings"
	"text/template"
)

// HookManager manages Slurm prolog/epilog hooks.
type HookManager struct {
	config      SlurmConfig
	slurmClient *Client
	aamiBin     string
}

// NewHookManager creates a new hook manager.
func NewHookManager(cfg SlurmConfig, client *Client) *HookManager {
	return &HookManager{
		config:      cfg,
		slurmClient: client,
		aamiBin:     "/usr/local/bin/aami",
	}
}

// SetAAMIBinary sets the path to the AAMI binary.
func (h *HookManager) SetAAMIBinary(path string) {
	h.aamiBin = path
}

// InstallHooks installs Slurm prolog/epilog scripts.
func (h *HookManager) InstallHooks(prologPath, epilogPath string) error {
	if h.config.PreJobCheck {
		if err := h.installProlog(prologPath); err != nil {
			return fmt.Errorf("install prolog: %w", err)
		}
	}

	if h.config.PostJobCheck {
		if err := h.installEpilog(epilogPath); err != nil {
			return fmt.Errorf("install epilog: %w", err)
		}
	}

	return nil
}

// HookConfig contains configuration for hook generation.
type HookConfig struct {
	AAMIBin         string
	HealthThreshold int
	AutoDrain       bool
	DrainOnXid      []int
	LogPath         string
}

const prologTemplate = `#!/bin/bash
# AAMI Pre-Job GPU Health Check
# Generated by: aami slurm install-hooks
# Description: Checks GPU health before job starts, rejects if unhealthy
#
# This script is called by Slurm before each job starts.
# It queries AAMI for GPU health and may reject the job if GPUs are unhealthy.

set -euo pipefail

AAMI_BIN="{{ .AAMIBin }}"
HEALTH_THRESHOLD="{{ .HealthThreshold }}"
LOG_FILE="{{ .LogPath }}/aami-prolog.log"

# Get node and job info from Slurm environment
NODE=$(hostname)
JOB_ID=${SLURM_JOB_ID:-0}
JOB_USER=${SLURM_JOB_USER:-unknown}

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] [JOB:$JOB_ID] [NODE:$NODE] $1" >> "$LOG_FILE"
}

# Ensure log directory exists
mkdir -p "$(dirname "$LOG_FILE")"

log "Prolog started for user=$JOB_USER"

# Check if AAMI is available
if [[ ! -x "$AAMI_BIN" ]]; then
    log "WARNING: AAMI binary not found at $AAMI_BIN, skipping health check"
    exit 0
fi

# Query GPU health
health_output=$("$AAMI_BIN" health "$NODE" --json 2>/dev/null) || {
    log "WARNING: Failed to get GPU health, allowing job to proceed"
    exit 0
}

# Parse health score
score=$(echo "$health_output" | jq -r '.overall // 100')

log "GPU health score: $score (threshold: $HEALTH_THRESHOLD)"

if [[ "$score" -lt "$HEALTH_THRESHOLD" ]]; then
    log "ERROR: GPU health below threshold, rejecting job"
    echo "ERROR: GPU health check failed on $NODE (score: $score, threshold: $HEALTH_THRESHOLD)" >&2
    echo "Please contact system administrators or try a different partition." >&2

    # Optionally drain the node
    {{- if .AutoDrain }}
    log "Auto-draining node due to low health score"
    "$AAMI_BIN" slurm drain "$NODE" --reason "AAMI: Low GPU health score ($score)" 2>/dev/null || true
    {{- end }}

    exit 1
fi

# Check for critical Xid errors
xid_check=$("$AAMI_BIN" health "$NODE" --json 2>/dev/null | jq -r '.components.xid.value // 0')
if [[ "$xid_check" != "0" && "$xid_check" != "null" ]]; then
    {{- if .DrainOnXid }}
    critical_xids="{{ range $i, $x := .DrainOnXid }}{{ if $i }} {{ end }}{{ $x }}{{ end }}"
    for xid in $critical_xids; do
        if echo "$xid_check" | grep -q "$xid"; then
            log "ERROR: Critical Xid $xid detected, rejecting job"
            echo "ERROR: Critical GPU error (Xid $xid) detected on $NODE" >&2
            "$AAMI_BIN" slurm drain "$NODE" --reason "AAMI: Critical Xid $xid" 2>/dev/null || true
            exit 1
        fi
    done
    {{- end }}
fi

log "Health check passed, allowing job to proceed"
exit 0
`

const epilogTemplate = `#!/bin/bash
# AAMI Post-Job GPU Health Check and Correlation
# Generated by: aami slurm install-hooks
# Description: Checks GPU health after job, logs correlation data
#
# This script is called by Slurm after each job completes.
# It queries AAMI for GPU health and logs correlation data for failed jobs.

set -euo pipefail

AAMI_BIN="{{ .AAMIBin }}"
HEALTH_THRESHOLD="{{ .HealthThreshold }}"
LOG_FILE="{{ .LogPath }}/aami-epilog.log"

# Get node and job info from Slurm environment
NODE=$(hostname)
JOB_ID=${SLURM_JOB_ID:-0}
JOB_USER=${SLURM_JOB_USER:-unknown}
EXIT_CODE=${SLURM_JOB_EXIT_CODE:-0}

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] [JOB:$JOB_ID] [NODE:$NODE] $1" >> "$LOG_FILE"
}

# Ensure log directory exists
mkdir -p "$(dirname "$LOG_FILE")"

log "Epilog started for user=$JOB_USER exit_code=$EXIT_CODE"

# Check if AAMI is available
if [[ ! -x "$AAMI_BIN" ]]; then
    log "WARNING: AAMI binary not found, skipping post-job check"
    exit 0
fi

# Query GPU health
health_output=$("$AAMI_BIN" health "$NODE" --json 2>/dev/null) || {
    log "WARNING: Failed to get GPU health"
    exit 0
}

score=$(echo "$health_output" | jq -r '.overall // 100')
log "Post-job GPU health score: $score"

# If job failed and GPU health is low, log correlation
if [[ "$EXIT_CODE" -ne 0 ]] && [[ "$score" -lt 70 ]]; then
    log "Potential GPU-related failure detected (exit=$EXIT_CODE, health=$score)"

    # Log correlation data
    "$AAMI_BIN" slurm log-correlation \
        --job "$JOB_ID" \
        --node "$NODE" \
        --score "$score" \
        --exit-code "$EXIT_CODE" 2>/dev/null || true
fi

# Auto-drain if health is critical
{{- if .AutoDrain }}
if [[ "$score" -lt "$HEALTH_THRESHOLD" ]]; then
    log "Health critical ($score < $HEALTH_THRESHOLD), draining node"
    "$AAMI_BIN" slurm drain "$NODE" \
        --reason "AAMI: GPU health degraded after job $JOB_ID (score: $score)" 2>/dev/null || true
fi
{{- end }}

# Check for new Xid errors
{{- if .DrainOnXid }}
xid_check=$("$AAMI_BIN" health "$NODE" --json 2>/dev/null | jq -r '.components.xid.recent // []' | jq -r '.[]')
if [[ -n "$xid_check" ]]; then
    critical_xids="{{ range $i, $x := .DrainOnXid }}{{ if $i }} {{ end }}{{ $x }}{{ end }}"
    for xid in $critical_xids; do
        if echo "$xid_check" | grep -q "$xid"; then
            log "Critical Xid $xid detected after job, draining node"
            "$AAMI_BIN" slurm drain "$NODE" \
                --reason "AAMI: Xid $xid after job $JOB_ID" 2>/dev/null || true
            break
        fi
    done
fi
{{- end }}

log "Epilog completed"
exit 0
`

func (h *HookManager) installProlog(path string) error {
	return h.installScript(path, prologTemplate)
}

func (h *HookManager) installEpilog(path string) error {
	return h.installScript(path, epilogTemplate)
}

func (h *HookManager) installScript(path, templateStr string) error {
	// Ensure directory exists
	dir := filepath.Dir(path)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("create directory: %w", err)
	}

	tmpl, err := template.New("hook").Parse(templateStr)
	if err != nil {
		return fmt.Errorf("parse template: %w", err)
	}

	config := HookConfig{
		AAMIBin:         h.aamiBin,
		HealthThreshold: h.config.HealthThreshold,
		AutoDrain:       h.config.AutoDrain,
		DrainOnXid:      h.config.DrainOnXid,
		LogPath:         "/var/log/aami",
	}

	f, err := os.OpenFile(path, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, 0755)
	if err != nil {
		return fmt.Errorf("create file: %w", err)
	}
	defer f.Close()

	if err := tmpl.Execute(f, config); err != nil {
		return fmt.Errorf("execute template: %w", err)
	}

	return nil
}

// GenerateSlurmConf generates slurm.conf snippet for hooks.
func (h *HookManager) GenerateSlurmConf(prologPath, epilogPath string) string {
	var sb strings.Builder

	sb.WriteString("# AAMI GPU Health Hooks\n")
	sb.WriteString("# Add these lines to slurm.conf and restart slurmctld\n\n")

	if h.config.PreJobCheck {
		sb.WriteString(fmt.Sprintf("Prolog=%s\n", prologPath))
	}

	if h.config.PostJobCheck {
		sb.WriteString(fmt.Sprintf("Epilog=%s\n", epilogPath))
	}

	sb.WriteString("\n# Optional: Set timeout for hooks (default 30 seconds)\n")
	sb.WriteString("# PrologTimeout=60\n")
	sb.WriteString("# EpilogTimeout=60\n")

	return sb.String()
}

// UninstallHooks removes installed hooks.
func (h *HookManager) UninstallHooks(prologPath, epilogPath string) error {
	var errs []string

	if err := os.Remove(prologPath); err != nil && !os.IsNotExist(err) {
		errs = append(errs, fmt.Sprintf("remove prolog: %v", err))
	}

	if err := os.Remove(epilogPath); err != nil && !os.IsNotExist(err) {
		errs = append(errs, fmt.Sprintf("remove epilog: %v", err))
	}

	if len(errs) > 0 {
		return fmt.Errorf("uninstall errors: %s", strings.Join(errs, "; "))
	}

	return nil
}

// ValidateHooks checks if hooks are properly installed and executable.
func (h *HookManager) ValidateHooks(prologPath, epilogPath string) []string {
	var issues []string

	if h.config.PreJobCheck {
		if err := validateHookFile(prologPath); err != nil {
			issues = append(issues, fmt.Sprintf("prolog: %v", err))
		}
	}

	if h.config.PostJobCheck {
		if err := validateHookFile(epilogPath); err != nil {
			issues = append(issues, fmt.Sprintf("epilog: %v", err))
		}
	}

	return issues
}

func validateHookFile(path string) error {
	info, err := os.Stat(path)
	if err != nil {
		if os.IsNotExist(err) {
			return fmt.Errorf("file not found: %s", path)
		}
		return err
	}

	// Check if executable
	if info.Mode()&0111 == 0 {
		return fmt.Errorf("file not executable: %s", path)
	}

	return nil
}

// TaskPrologScript generates a task prolog (per-task, not per-job).
const taskPrologTemplate = `#!/bin/bash
# AAMI Task Prolog - Per-GPU check before each task
# Use TaskProlog in slurm.conf to run this before each task

AAMI_BIN="{{ .AAMIBin }}"
NODE=$(hostname)
TASK_ID=${SLURM_LOCALID:-0}

# Get assigned GPU for this task
if [[ -n "${CUDA_VISIBLE_DEVICES:-}" ]]; then
    GPU_ID=$(echo "$CUDA_VISIBLE_DEVICES" | cut -d',' -f1)

    # Quick health check for specific GPU
    health=$("$AAMI_BIN" health "$NODE" --gpu "$GPU_ID" --json 2>/dev/null | jq -r '.score // 100')

    if [[ "$health" -lt 50 ]]; then
        echo "WARNING: GPU $GPU_ID health is low ($health)" >&2
    fi
fi

exit 0
`

// GenerateTaskProlog generates a task-level prolog script.
func (h *HookManager) GenerateTaskProlog(path string) error {
	tmpl, err := template.New("taskprolog").Parse(taskPrologTemplate)
	if err != nil {
		return err
	}

	config := HookConfig{
		AAMIBin: h.aamiBin,
	}

	f, err := os.OpenFile(path, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, 0755)
	if err != nil {
		return err
	}
	defer f.Close()

	return tmpl.Execute(f, config)
}
