# Alerting System Architecture

## Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Components](#components)
4. [Alert Types](#alert-types)
5. [Data Flow](#data-flow)
6. [Group-based Customization](#group-based-customization)
7. [Alert Rule Generation](#alert-rule-generation)
8. [Integration Points](#integration-points)
9. [Examples](#examples)
10. [FAQ](#faq)

---

## Overview

AAMI's alerting system provides comprehensive monitoring and notification capabilities for AI accelerator infrastructure. The system is built on Prometheus and Alertmanager, offering a unified alert path that handles both standard metric-based alerts and custom check-based alerts.

### Key Features

- **Unified Alert Path**: All alerts flow through Prometheus ‚Üí Alertmanager
- **Group-based Customization**: Different alert thresholds per group/namespace
- **Label-based Filtering**: Precise targeting of alerts to specific infrastructure
- **Dynamic Check System**: Script-based monitoring for custom requirements
- **Template-based Management**: Reusable alert and check templates
- **Policy Inheritance**: Smart configuration merging across group hierarchy

### Design Philosophy

AAMI maintains a **single, consistent alerting pipeline** rather than multiple independent notification systems. This approach provides:

- Centralized alert management
- Consistent routing and grouping policies
- Unified notification channels
- Easier troubleshooting and debugging
- Predictable alert behavior

---

## Architecture

### System Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AI Accelerator Cluster                        ‚îÇ
‚îÇ              (GPU Servers, Storage, Network)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ                      ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Node Exporter    ‚îÇ  ‚îÇ Custom Checks   ‚îÇ
       ‚îÇ  DCGM Exporter    ‚îÇ  ‚îÇ (dynamic-check) ‚îÇ
       ‚îÇ  Custom Exporters ‚îÇ  ‚îÇ                 ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ                      ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ Metrics
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ    Prometheus       ‚îÇ
                 ‚îÇ  - Scrape metrics   ‚îÇ
                 ‚îÇ  - Evaluate rules   ‚îÇ
                 ‚îÇ  - Store TSDB       ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ Firing Alerts
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ   Alertmanager      ‚îÇ
                 ‚îÇ  - Route alerts     ‚îÇ
                 ‚îÇ  - Group/Inhibit    ‚îÇ
                 ‚îÇ  - Deduplicate      ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ Notifications
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                  ‚îÇ                  ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Email  ‚îÇ      ‚îÇ   Slack    ‚îÇ     ‚îÇ Webhook  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Unified Alert Path

**Critical Design Decision**: All alerts, regardless of source, follow the same path:

```
Source ‚Üí Metrics ‚Üí Prometheus ‚Üí Alert Rules ‚Üí Alertmanager ‚Üí Notification
```

This means:
- ‚ùå No direct email sending from check scripts
- ‚ùå No independent notification systems
- ‚úÖ All alerts through Prometheus/Alertmanager
- ‚úÖ Consistent routing and grouping
- ‚úÖ Centralized configuration

---

## Components

### Prometheus

**Role**: Metrics collection, storage, and alert rule evaluation

**Responsibilities**:
- Scrape metrics from exporters every 15 seconds (configurable)
- Store time-series data in TSDB
- Evaluate alert rules every 15 seconds (configurable)
- Send firing alerts to Alertmanager
- Provide PromQL query interface

**Configuration**:
- `config/prometheus/prometheus.yml`: Main configuration
- `config/prometheus/rules/*.yml`: Alert rules
- Service Discovery via HTTP SD from Config Server

### Alertmanager

**Role**: Alert management and routing

**Responsibilities**:
- **Routing**: Direct alerts to appropriate receivers based on labels
- **Grouping**: Combine similar alerts to reduce notification volume
- **Inhibition**: Suppress lower-priority alerts when higher-priority ones fire
- **Deduplication**: Prevent duplicate notifications
- **Silencing**: Temporarily mute specific alerts

**Configuration**: `config/alertmanager/alertmanager.yml`

**Key Features**:
- Severity-based routing (critical, warning, info)
- Namespace-based routing (infrastructure, logical, environment)
- Time-based grouping (group_wait, group_interval, repeat_interval)

### Alert Rules

**Role**: Define conditions that trigger alerts

**Structure**:
```yaml
- alert: AlertName
  expr: PromQL expression
  for: duration
  labels:
    severity: critical
    group_id: grp-123
  annotations:
    summary: Alert summary
    description: Detailed description
```

**Storage**: `config/prometheus/rules/*.yml`

**Current State**:
- ‚úÖ Static rule files (manually created)
- üìã Dynamic generation (planned for Phase 3)

---

## Alert Types

### 1. Prometheus-based Alerts

**Definition**: Alerts triggered by standard Prometheus metrics from exporters

**Data Flow**:
```
Exporter ‚Üí Prometheus ‚Üí Alert Rules ‚Üí Alertmanager
```

**Examples**:
- Node Exporter metrics: CPU, memory, disk, network
- DCGM Exporter metrics: GPU utilization, temperature, power
- Custom Exporter metrics: Application-specific

**Rule Example**:
```yaml
- alert: HighCPUUsage
  expr: |
    (100 - (avg by(instance) (
      rate(node_cpu_seconds_total{mode="idle"}[5m])
    ) * 100)) > 80
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "High CPU usage on {{ $labels.instance }}"
```

### 2. Custom Check System

**Definition**: Script-based monitoring for infrastructure components not covered by standard exporters

**Data Flow**:
```
Config Server (CheckTemplate/Instance)
  ‚Üì
Node queries effective checks
  ‚Üì
dynamic-check.sh executes scripts
  ‚Üì
JSON output
  ‚Üì
Convert to Prometheus format
  ‚Üì
Save to /var/lib/node_exporter/textfile/*.prom
  ‚Üì
Node Exporter textfile collector
  ‚Üì
Prometheus scrapes
  ‚Üì
Alert Rules evaluate
  ‚Üì
Alertmanager
```

**Use Cases**:
- Mount point availability
- Device connection status
- Network interface checks
- Custom application health checks
- Filesystem-specific monitoring

**Key Components**:
- **CheckTemplate**: Reusable script definition (services/config-server/internal/domain/check_template.go)
- **CheckInstance**: Group-specific application (services/config-server/internal/domain/check_instance.go)
- **Scope-based Management**: Global ‚Üí Namespace ‚Üí Group hierarchy

**Example**: Mount Point Check

```bash
# CheckTemplate script
#!/bin/bash
PATHS="$1"
for path in ${PATHS//,/ }; do
  if mountpoint -q "$path"; then
    echo '{"name":"mount_status","value":1,"labels":{"path":"'$path'"}}'
  else
    echo '{"name":"mount_status","value":0,"labels":{"path":"'$path'"}}'
  fi
done
```

Output to textfile:
```
mount_status{path="/data"} 1
mount_status{path="/mnt/models"} 0
```

Alert rule:
```yaml
- alert: MountPointUnavailable
  expr: mount_status == 0
  for: 2m
  labels:
    severity: critical
  annotations:
    summary: "Mount point {{ $labels.path }} unavailable"
```

**Important**: Custom checks still go through Prometheus/Alertmanager, not direct notification.

---

## Data Flow

### Standard Metrics Path

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node Exporter   ‚îÇ  Port 9100, metrics endpoint
‚îÇ DCGM Exporter   ‚îÇ  Port 9400, metrics endpoint
‚îÇ Custom Exporter ‚îÇ  Port 9xxx, metrics endpoint
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ HTTP GET /metrics (every 15s)
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Prometheus                     ‚îÇ
‚îÇ - Scrape metrics               ‚îÇ
‚îÇ - Store in TSDB                ‚îÇ
‚îÇ - Evaluate rules (every 15s)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ Firing alerts
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Alertmanager                   ‚îÇ
‚îÇ - Route by severity/namespace  ‚îÇ
‚îÇ - Group similar alerts         ‚îÇ
‚îÇ - Apply inhibition rules       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ Send notifications
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Notification Channels          ‚îÇ
‚îÇ - Email (SMTP)                 ‚îÇ
‚îÇ - Slack (Webhook)              ‚îÇ
‚îÇ - PagerDuty (Webhook)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Custom Check Path

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Config Server                   ‚îÇ
‚îÇ - CheckTemplate storage         ‚îÇ
‚îÇ - CheckInstance management      ‚îÇ
‚îÇ - Scope resolution              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ GET /api/v1/checks/node?hostname=gpu-node-01
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node: dynamic-check.sh          ‚îÇ
‚îÇ 1. Query effective checks       ‚îÇ
‚îÇ 2. Execute scripts              ‚îÇ
‚îÇ 3. Collect JSON output          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ JSON metrics
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Convert to Prometheus format    ‚îÇ
‚îÇ mount_status{path="/data"} 1    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ Write to file
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ /var/lib/node_exporter/         ‚îÇ
‚îÇ   textfile/*.prom               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ Read by textfile collector
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node Exporter                   ‚îÇ
‚îÇ - Expose as metrics             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ Scrape
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Prometheus                      ‚îÇ
‚îÇ (Same path as standard metrics) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Group-based Customization

### Problem Statement

**Question**: Prometheus alert rules are global. How can we support different alert thresholds per group?

**Example**:
- Production group: CPU alert at 80%
- Development group: CPU alert at 95%

### Solution: Label-based Filtering + Dynamic Rule Generation

#### Step 1: Add Group Labels in Service Discovery

**Code**: `services/config-server/internal/domain/service_discovery.go:38-54`

```go
// When registering targets, add group information as labels
labels["group"] = target.Groups[0].Name           // "gpu-cluster-a"
labels["group_id"] = target.Groups[0].ID          // "grp-123"
labels["namespace"] = target.Groups[0].Namespace.Name  // "production"
```

**Result**: All metrics from this target include group labels

```promql
node_cpu_seconds_total{
  instance="gpu-node-01",
  group="gpu-cluster-a",
  group_id="grp-123",
  namespace="production"
}
```

#### Step 2: Generate Group-specific Alert Rules

Each group gets its own alert rule with:
- Group-specific PromQL filter (`group_id="grp-123"`)
- Group-specific threshold (80% vs 95%)
- Group-specific duration (5m vs 10m)

**Production Group** (threshold: 80%):
```yaml
# /etc/prometheus/rules/generated/production-group-grp-123.yml
groups:
  - name: production_cpu_alerts
    rules:
      - alert: HighCPUUsage_Production
        expr: |
          (100 - (avg by(instance) (
            rate(node_cpu_seconds_total{
              mode="idle",
              group_id="grp-123"  # Filter to this group
            }[5m])
          ) * 100)) > 80  # Production threshold
        for: 5m
        labels:
          severity: warning
          group_id: grp-123
          namespace: production
```

**Development Group** (threshold: 95%):
```yaml
# /etc/prometheus/rules/generated/development-group-grp-456.yml
groups:
  - name: development_cpu_alerts
    rules:
      - alert: HighCPUUsage_Development
        expr: |
          (100 - (avg by(instance) (
            rate(node_cpu_seconds_total{
              mode="idle",
              group_id="grp-456"  # Filter to this group
            }[5m])
          ) * 100)) > 95  # Development threshold
        for: 10m
        labels:
          severity: info
          group_id: grp-456
          namespace: development
```

#### Step 3: AlertRule.RenderQuery() for Dynamic Generation

**Code**: `services/config-server/internal/domain/alert.go:102-125`

```go
// AlertTemplate with query template
QueryTemplate: `(100 - avg(rate(node_cpu_seconds_total{
  mode="idle",
  group_id="{{.group_id}}"
}[5m])) * 100) > {{.threshold}}`

// Production Group Config
Config: {
  "group_id": "grp-123",
  "threshold": 80,
  "for_duration": "5m"
}

// Rendered PromQL:
"(100 - avg(rate(node_cpu_seconds_total{
  mode=\"idle\",
  group_id=\"grp-123\"
}[5m])) * 100) > 80"
```

### Benefits

- ‚úÖ Same metric, different thresholds per group
- ‚úÖ Clean separation of rules by group
- ‚úÖ Easy to debug (group_id in labels)
- ‚úÖ Scalable (automatic generation)
- ‚úÖ Flexible (template + config approach)

### Target-specific Customization

For even finer control, use target labels:

```yaml
- alert: HighCPUUsage_GPU_Servers
  expr: |
    (100 - avg by(instance) (
      rate(node_cpu_seconds_total{
        mode="idle",
        group_id="grp-123",
        target_label_type="gpu"  # Target-specific filter
      }[5m])
    ) * 100) > 70  # Different threshold for GPU servers
```

---

## Alert Rule Generation

### Current State

**Implemented**:
- ‚úÖ AlertTemplate API (services/config-server/internal/service/alert.go)
- ‚úÖ AlertRule API (group-specific configuration)
- ‚úÖ AlertRule.RenderQuery() (template rendering)
- ‚úÖ Group hierarchy and policy inheritance
- ‚úÖ Database schema (alert_templates, alert_rules)

**Not Implemented**:
- ‚ùå Prometheus rule file generation
- ‚ùå Dynamic rule deployment to Prometheus
- ‚ùå Automatic Prometheus reload

### Planned Implementation (Phase 3)

**Location**: `services/config-server/internal/service/prometheus_rule_generator.go` (future)

**Workflow**:
```go
func GeneratePrometheusRules(ctx context.Context) error {
    // 1. Query all enabled alert rules from database
    rules := alertRuleRepo.ListEnabled(ctx)

    // 2. Group by group_id
    rulesByGroup := groupRules(rules)

    // 3. For each group, generate rule file
    for groupID, groupRules := range rulesByGroup {
        prometheusRules := []PrometheusRule{}

        for _, rule := range groupRules {
            // 4. Render PromQL query (already implemented!)
            query := rule.RenderQuery()

            // 5. Convert to Prometheus YAML format
            prometheusRules = append(prometheusRules, PrometheusRule{
                Alert: fmt.Sprintf("%s_Group_%s", rule.Name, groupID),
                Expr:  query,
                For:   rule.Config["for_duration"],
                Labels: map[string]string{
                    "group_id": groupID,
                    "severity": string(rule.Severity),
                },
            })
        }

        // 6. Write to file
        filename := fmt.Sprintf("/etc/prometheus/rules/generated/group-%s.yml", groupID)
        writeYAML(filename, prometheusRules)
    }

    // 7. Reload Prometheus
    reloadPrometheus()
}
```

**Trigger Events**:
- Alert rule created/updated/deleted
- Group configuration changed
- Manual refresh via API

**Expected Timeline**: Q2 2025 (Phase 3: Integration & Advanced Features)

---

## Integration Points

### 1. Service Discovery ‚Üí Labels

**File**: `services/config-server/internal/domain/service_discovery.go`

When targets are registered, group information is added as labels:

```go
labels["group"] = target.Groups[0].Name
labels["group_id"] = target.Groups[0].ID
labels["namespace"] = target.Groups[0].Namespace.Name
```

These labels are used in:
- Alert rule filtering (`group_id="grp-123"`)
- Alertmanager routing (`namespace: production`)
- Grafana dashboard variables

### 2. Alert Rules ‚Üí Alertmanager

**File**: `config/prometheus/prometheus.yml:8-12`

```yaml
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093
```

Prometheus sends firing alerts to Alertmanager with all labels preserved.

### 3. Alertmanager ‚Üí Notification Channels

**File**: `config/alertmanager/alertmanager.yml`

Routes alerts based on:
- **Severity**: critical, warning, info
- **Namespace**: infrastructure, logical, environment
- **Custom labels**: team, service, etc.

Example routing:
```yaml
routes:
  - match:
      severity: critical
    receiver: 'oncall-team'
    group_wait: 0s
    repeat_interval: 4h

  - match:
      namespace: infrastructure
    receiver: 'infrastructure-team'
    continue: true
```

### 4. CheckInstance ‚Üí Node Execution

**API Endpoint**: `GET /api/v1/checks/node?hostname={hostname}`

Nodes query Config Server to get:
- Effective CheckInstances (after scope resolution)
- Script content and hash
- Merged configuration

Response:
```json
[
  {
    "check_type": "mount",
    "script_content": "#!/bin/bash\n...",
    "config": {
      "paths": "/data,/mnt/models"
    },
    "hash": "abc123..."
  }
]
```

---

## Examples

### Example 1: Standard Metric Alert (Node Down)

**Rule File**: `config/prometheus/rules/system-alerts.yml`

```yaml
- alert: NodeDown
  expr: up{job="node-exporter"} == 0
  for: 2m
  labels:
    severity: critical
    namespace: infrastructure
  annotations:
    summary: "Node {{ $labels.instance }} is down"
    description: |
      Node has not responded for more than 2 minutes.
      Instance: {{ $labels.instance }}
      Primary Group: {{ $labels.group }}
```

**Flow**:
1. Node Exporter stops responding
2. Prometheus marks `up{job="node-exporter"}` as 0
3. Alert rule condition met for 2 minutes
4. Prometheus sends alert to Alertmanager
5. Alertmanager routes to 'critical-alerts' receiver (email + PagerDuty)

### Example 2: Group-specific Disk Alert

**Scenario**: Different disk thresholds for different environments

**AlertTemplate**:
```json
{
  "name": "HighDiskUsage",
  "query_template": "((node_filesystem_avail_bytes{group_id=\"{{.group_id}}\"} / node_filesystem_size_bytes) * 100) < {{.threshold}}",
  "default_config": {
    "threshold": 20
  }
}
```

**AlertRule (Production)**:
```json
{
  "group_id": "production-grp-123",
  "template_id": "HighDiskUsage",
  "config": {
    "threshold": 20,
    "for_duration": "5m"
  }
}
```

**AlertRule (Development)**:
```json
{
  "group_id": "development-grp-456",
  "template_id": "HighDiskUsage",
  "config": {
    "threshold": 10,
    "for_duration": "10m"
  }
}
```

**Generated Prometheus Rules**:

Production:
```yaml
- alert: HighDiskUsage_Production
  expr: ((node_filesystem_avail_bytes{group_id="production-grp-123"} / node_filesystem_size_bytes) * 100) < 20
  for: 5m
```

Development:
```yaml
- alert: HighDiskUsage_Development
  expr: ((node_filesystem_avail_bytes{group_id="development-grp-456"} / node_filesystem_size_bytes) * 100) < 10
  for: 10m
```

### Example 3: Custom Check (Mount Point)

**CheckTemplate**:
```bash
POST /api/v1/check-templates
{
  "name": "mount-check",
  "check_type": "mount",
  "script_content": "#!/bin/bash\nPATHS=\"$1\"\nfor path in ${PATHS//,/ }; do\n  if mountpoint -q \"$path\"; then\n    echo '{\"name\":\"mount_status\",\"value\":1,\"labels\":{\"path\":\"'$path'\"}}'\n  else\n    echo '{\"name\":\"mount_status\",\"value\":0,\"labels\":{\"path\":\"'$path'\"}}'\n  fi\ndone",
  "language": "bash",
  "default_config": {
    "paths": "/data"
  }
}
```

**CheckInstance (ML Training Group)**:
```bash
POST /api/v1/check-instances
{
  "template_id": "mount-check-template-id",
  "scope": "group",
  "group_id": "ml-training-group",
  "config": {
    "paths": "/data,/mnt/models,/mnt/datasets"
  }
}
```

**Node Execution**:
```bash
# dynamic-check.sh runs periodically
/opt/aami/scripts/dynamic-check.sh

# Outputs to textfile:
# /var/lib/node_exporter/textfile/mount-check.prom
mount_status{path="/data"} 1
mount_status{path="/mnt/models"} 0  # Failed!
mount_status{path="/mnt/datasets"} 1
```

**Alert Rule**:
```yaml
- alert: MountPointUnavailable
  expr: mount_status == 0
  for: 2m
  labels:
    severity: critical
  annotations:
    summary: "Mount point {{ $labels.path }} unavailable on {{ $labels.instance }}"
```

**Result**: When `/mnt/models` fails to mount, alert fires after 2 minutes and sends notification.

---

## FAQ

### Q: Does the alert system depend on Alertmanager?

**A**: Partially.

- **Alert Evaluation**: No dependency. Prometheus evaluates alert rules independently and marks alerts as "firing" in its internal state.
- **Alert Notification**: Yes, requires Alertmanager. Without it, alerts are visible in Prometheus UI (`http://localhost:9090/alerts`) but no notifications are sent.

**With Alertmanager**:
```
Prometheus ‚Üí Evaluates rules ‚Üí Fires alerts ‚Üí Alertmanager ‚Üí Email/Slack
```

**Without Alertmanager**:
```
Prometheus ‚Üí Evaluates rules ‚Üí Fires alerts ‚Üí [No notifications]
                                             ‚îî‚Üí Visible in Prometheus UI only
```

### Q: Are Prometheus alert rules global?

**A**: Yes, but AAMI uses **label-based filtering** to achieve group-specific behavior.

- Prometheus rule files are global (loaded from `config/prometheus/rules/*.yml`)
- Each rule can filter metrics by labels (`group_id="grp-123"`)
- AAMI generates separate rules for each group with different thresholds
- Result: Appears group-specific, implemented as multiple global rules

### Q: Is custom infrastructure monitoring done via custom exporters?

**A**: No, AAMI uses the **Custom Check System**, not custom exporters.

**Custom Exporter** (traditional approach):
- Separate Go/Python process
- Exposes HTTP metrics endpoint
- Requires binary deployment
- Hard to customize per group

**AAMI Check System** (dynamic approach):
- Shell/Python scripts
- JSON output ‚Üí Prometheus format conversion
- Node Exporter textfile collector
- Easy per-group customization via CheckInstance
- Dynamic deployment via Config Server API

Both paths eventually go through Prometheus ‚Üí Alertmanager.

### Q: Can alerts bypass Prometheus/Alertmanager for faster notification?

**A**: No, and this is by design.

**Why unified path**:
- Consistent alert routing and grouping
- Single source of truth for alert state
- Easier troubleshooting (one place to check)
- Alertmanager features (inhibition, deduplication, silencing)
- Prevents alert storms from multiple sources

**Trade-off**:
- Slight delay (scrape_interval + evaluation_interval + Alertmanager processing)
- Typical latency: 30-60 seconds
- Acceptable for infrastructure monitoring
- For sub-second requirements, consider direct monitoring in application code

### Q: How do I test alert rules before deployment?

**A**: Use Prometheus UI and promtool:

```bash
# Validate syntax
promtool check rules config/prometheus/rules/system-alerts.yml

# Test query in Prometheus UI
http://localhost:9090/graph

# Enter PromQL expression
(100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80

# Manually trigger alert (set threshold very low)
# Watch Alerts page
http://localhost:9090/alerts
```

### Q: Can I have both global and group-specific alert rules?

**A**: Yes, this is a common pattern.

**Global Rule** (baseline for all groups):
```yaml
- alert: NodeDown
  expr: up{job="node-exporter"} == 0
  for: 5m  # More lenient
```

**Group-specific Rule** (stricter for production):
```yaml
- alert: NodeDown_Production
  expr: up{job="node-exporter",namespace="production"} == 0
  for: 1m  # Faster alert for production
```

Use inhibition rules to prevent duplicate alerts.

### Q: What happens when an alert rule file has syntax errors?

**A**: Prometheus will:
1. Log error on startup/reload
2. Skip the invalid rule file
3. Continue with valid rule files
4. Alert evaluation continues for valid rules

Always validate with `promtool check rules` before deployment.

### Q: How do I silence alerts during maintenance?

**A**: Use Alertmanager silences:

```bash
# Via UI
http://localhost:9093/#/silences

# Via API
curl -X POST http://localhost:9093/api/v2/silences \
  -H "Content-Type: application/json" \
  -d '{
    "matchers": [
      {"name": "instance", "value": "gpu-node-01", "isRegex": false}
    ],
    "startsAt": "2025-01-01T10:00:00Z",
    "endsAt": "2025-01-01T12:00:00Z",
    "comment": "Scheduled maintenance",
    "createdBy": "admin@example.com"
  }'
```

Silences are temporary and automatically expire.

---

## References

- [Check Management System](./CHECK-MANAGEMENT.md) - Custom check system details
- [Quick Start Guide](./QUICKSTART.md) - Getting started with AAMI
- [API Documentation](./API.md) - Alert and check API reference
- [Prometheus Documentation](https://prometheus.io/docs/alerting/latest/overview/)
- [Alertmanager Documentation](https://prometheus.io/docs/alerting/latest/alertmanager/)
