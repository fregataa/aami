# AAMI Prometheus Federation Configuration Template
# This file serves as a template and documentation for federation setup.
# The actual configuration is generated by 'aami federation enable'

# Federation is recommended for clusters with 500+ nodes
# It splits monitoring across multiple Prometheus instances (shards)

# ============================================================
# Federation Architecture
# ============================================================
#
#   ┌──────────────────────────────────────────────────────┐
#   │                 Central Prometheus                    │
#   │              (localhost:9090)                         │
#   │                                                       │
#   │  • Federates from all shards every 60s              │
#   │  • Stores aggregated metrics (2d raw, 30d downsampled)│
#   │  • Evaluates alert rules                             │
#   │  • Connected to Alertmanager                         │
#   └───────────────┬────────────────┬────────────────────┘
#                   │                │
#         ┌─────────▼──────┐  ┌──────▼─────────┐
#         │   Shard 1      │  │    Shard 2     │  ...
#         │ (localhost:9091)│  │(localhost:9092)│
#         │                │  │                │
#         │ • 200 nodes    │  │ • 200 nodes    │
#         │ • 7d retention │  │ • 7d retention │
#         └────────────────┘  └────────────────┘
#
# ============================================================

# Sharding Strategies
# --------------------
#
# 1. auto (default): Evenly distribute nodes across shards
#    - Recommended for most cases
#    - Balances load automatically
#
# 2. rack: Distribute by rack label
#    - Use when nodes have 'rack' label in config
#    - Keeps rack-local metrics together
#    - Good for data locality
#
# 3. count: Fixed nodes per shard
#    - Specify exact number of nodes per shard
#    - Use when you need predictable distribution

# Example Configuration
# ---------------------
federation:
  enabled: true
  type: prometheus  # or 'thanos' for long-term storage

  # Central Prometheus configuration
  central:
    port: 9090
    retention_raw: "2d"          # Keep raw federated data
    retention_downsampled: "30d" # Keep aggregated data longer
    federate_interval: "60s"     # How often to pull from shards
    storage_path: "/var/lib/aami/prometheus-central"

  # Shard configurations
  shards:
    - name: "shard-1"
      nodes:
        - "gpu-node-001"
        - "gpu-node-002"
        # ... up to ~200 nodes per shard
      prometheus:
        port: 9091
        storage_path: "/var/lib/aami/prometheus-shard-1"
        retention: "7d"

    - name: "shard-2"
      nodes:
        - "gpu-node-201"
        - "gpu-node-202"
      prometheus:
        port: 9092
        storage_path: "/var/lib/aami/prometheus-shard-2"
        retention: "7d"

    # For rack-based sharding:
    - name: "shard-rack-a"
      racks:
        - "rack-a"
      prometheus:
        port: 9093
        storage_path: "/var/lib/aami/prometheus-shard-rack-a"
        retention: "7d"

# Sizing Guidelines
# -----------------
#
# | Node Count | Recommended Shards | Notes                    |
# |------------|-------------------|--------------------------|
# | < 100      | 1 (no federation) | Use single Prometheus    |
# | 100-300    | 2                 | Basic federation         |
# | 300-500    | 3                 | Standard setup           |
# | 500-1000   | 5                 | Large cluster            |
# | 1000+      | 1 per 200 nodes   | Scale horizontally       |

# Resource Requirements per Shard
# --------------------------------
#
# Shard (200 nodes, 800 GPUs):
#   CPU: 2-4 cores
#   Memory: 8-16 GB
#   Storage: 50-100 GB SSD (7 day retention)
#
# Central:
#   CPU: 4-8 cores
#   Memory: 16-32 GB
#   Storage: 100-200 GB SSD

# Recording Rules (auto-generated)
# ---------------------------------
# When federation is enabled, these recording rules are created:
#
# Shard-level aggregations:
#   shard:DCGM_FI_DEV_GPU_UTIL:avg    - Average GPU utilization per shard
#   shard:DCGM_FI_DEV_FB_USED:sum     - Total memory used per shard
#   shard:gpu:count                    - GPU count per shard
#   shard:DCGM_FI_DEV_GPU_TEMP:max    - Max temperature per shard
#   shard:DCGM_FI_DEV_POWER_USAGE:sum - Total power per shard
#
# Cluster-level aggregations:
#   cluster:DCGM_FI_DEV_GPU_UTIL:avg  - Cluster-wide average utilization
#   cluster:gpu:count                  - Total GPUs in cluster
#   cluster:DCGM_FI_DEV_POWER_USAGE:sum - Total cluster power

# CLI Commands
# ------------
#
# Enable federation:
#   aami federation enable --shards 3
#   aami federation enable --by rack
#   aami federation enable --dry-run
#
# Check status:
#   aami federation status
#
# Manage shards:
#   aami federation shards
#   aami federation rebalance
#   aami federation validate
#
# Disable:
#   aami federation disable

# Thanos Integration (Optional)
# -----------------------------
# For long-term storage and global view across clusters:
#
# federation:
#   type: thanos
#   thanos:
#     sidecar_port: 10901
#     store_port: 10902
#     query_port: 10903
#     bucket:
#       type: s3
#       config:
#         bucket: "aami-metrics"
#         endpoint: "s3.amazonaws.com"
#         access_key: "${AWS_ACCESS_KEY}"
#         secret_key: "${AWS_SECRET_KEY}"
